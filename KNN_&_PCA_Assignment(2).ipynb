{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*1.  What is K-Nearest Neighbors (KNN) and how does it work ?*\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for classification and regression tasks. It's one of the simplest and most intuitive algorithms in machine learnin.\n",
        "\n",
        "KNN Works\n",
        "Training Phase:\n",
        "\n",
        "KNN doesn't explicitly learn a model during training (it's a lazy learner). It simply stores the training dataset.\n",
        "\n",
        "Prediction Phase:\n",
        "\n",
        "To classify (or predict) a new data point:\n",
        "\n",
        "Measure Distance: Calculate the distance (typically Euclidean) between the new data point and all the points in the training set.\n",
        "\n",
        "Find Neighbors: Select the K closest data points (neighbors) to the new point.\n",
        "\n",
        "Vote (for Classification):\n",
        "\n",
        "The new point is assigned the most common class among its K neighbors.\n",
        "\n",
        "Average (for Regression):\n",
        "\n",
        "The new point's value is the average of the values of its K nearest neighbors.\n",
        "\n",
        "*2.  What is the difference between KNN Classification and KNN Regression ?*\n",
        "\n",
        "The main difference between KNN Classification and KNN Regression lies in how they make predictions based on the nearest neighbors:\n",
        "\n",
        "KNN Classification\n",
        "Purpose: Assigns a class label to the input data point.\n",
        "\n",
        "Output: A categorical value (e.g., \"spam\" or \"not spam\").\n",
        "\n",
        "Prediction Method:\n",
        "\n",
        "Find the K nearest neighbors.\n",
        "\n",
        "Count the number of neighbors belonging to each class.\n",
        "\n",
        "Assign the class with the majority vote.\n",
        "\n",
        "Example:\n",
        "Classify a flower as setosa, versicolor, or virginica based on petal measurements.\n",
        "\n",
        "KNN Regression\n",
        "Purpose: Predicts a continuous value for the input data point.\n",
        "\n",
        "Output: A numerical value (e.g., price, temperature).\n",
        "\n",
        "Prediction Method:\n",
        "\n",
        "Find the K nearest neighbors.\n",
        "\n",
        "Compute the average (or weighted average) of their output values.\n",
        "\n",
        "Return this average as the prediction.\n",
        "\n",
        "Example:\n",
        "Predict the price of a house based on size, number of rooms, and location.\n",
        "\n",
        "*3.  What is the role of the distance metric in KNN ?*\n",
        "\n",
        "The distance metric in K-Nearest Neighbors (KNN) plays a crucial role in determining which training data points are considered \"nearest\" to a given test point. Since KNN relies on proximity to make predictions, the way distance is calculated directly impacts model accuracy.\n",
        "\n",
        "Role of the Distance Metric\n",
        "\n",
        "It measures similarity: Closer points are assumed to be more similar.\n",
        "\n",
        "It determines which K neighbors are selected.\n",
        "\n",
        "It affects both classification (via voting) and regression (via averaging).\n",
        "\n",
        "\n",
        "\n",
        "*4.  What is the Curse of Dimensionality in KNN?*\n",
        "\n",
        "The Curse of Dimensionality in K-Nearest Neighbors (KNN) refers to the problems that arise when working with data that has many features (high dimensions). As the number of dimensions increases, the performance and effectiveness of KNN (and many other machine learning algorithms) can degrade.\n",
        "\n",
        "The Curse of Dimensionality means that high-dimensional data can break KNN because distances become less meaningful, neighbors become less distinguishable, and the algorithm struggles to generalize well.\n",
        "\n",
        "*5.  How can we choose the best value of K in KNN ?*\n",
        "\n",
        "Choosing the best value of K in K-Nearest Neighbors (KNN) is critical for achieving optimal model performance. A poor choice can lead to overfitting or underfitting.\n",
        "\n",
        "How to Choose the Best K :\n",
        "\n",
        "1. Use Cross-Validation (Best Practice)\n",
        "Split your dataset into training and validation sets (e.g., using k-fold cross-validation).\n",
        "\n",
        "Try different values of K (e.g., 1 to 40).\n",
        "\n",
        "Measure the validation accuracy or error for each value.\n",
        "\n",
        "Choose the K with the best performance on the validation set.\n",
        "\n",
        "2. Use the Elbow Method (for visualization)\n",
        "Plot accuracy vs. K or error rate vs. K.\n",
        "\n",
        "Look for the \"elbow point\" where the accuracy levels off or error starts increasing.\n",
        "\n",
        "3. Heuristic Rules (Quick Estimates)\n",
        "Try odd values of K (to avoid ties in classification).\n",
        "\n",
        "A common starting point:\n",
        "\n",
        "ùêæ\n",
        "=\n",
        "ùëõ\n",
        "K=\n",
        "n\n",
        "‚Äã\n",
        "\n",
        "where\n",
        "ùëõ\n",
        "n = number of training samples.\n",
        "\n",
        "*6.  What are KD Tree and Ball Tree in KNN? *\n",
        "\n",
        "KD Tree and Ball Tree are data structures used to speed up the K-Nearest Neighbors (KNN) algorithm by efficiently organizing and searching high-dimensional data.\n",
        "\n",
        "They are especially useful when the dataset is large, as brute-force KNN (which compares every test point to all training points) becomes slow.\n",
        "\n",
        "KD Tree (K-Dimensional Tree)\n",
        "\n",
        "What is it :\n",
        "\n",
        "A binary tree that recursively splits the data along one dimension at a time.\n",
        "\n",
        "Designed for low to moderate-dimensional data (usually < 20 dimensions).\n",
        "\n",
        " How it works:\n",
        "At each level of the tree, the dataset is split by a median value along one axis (e.g., x, y, z...).\n",
        "\n",
        "Each node contains:\n",
        "\n",
        "A data point\n",
        "\n",
        "A splitting axis\n",
        "\n",
        "Pointers to left/right child nodes\n",
        "\n",
        " Use case:\n",
        "\n",
        "Faster nearest neighbor search in low-dimensional space.\n",
        "\n",
        "Used by scikit-learn's KNeighborsClassifier(algorithm='kd_tree').\n",
        "\n",
        " Limitation:\n",
        "\n",
        "Becomes inefficient in high dimensions due to the curse of dimensionality.\n",
        "\n",
        " 2. Ball Tree\n",
        "\n",
        " What is it\n",
        "\n",
        "A tree-based structure that partitions data into hyperspheres (balls) rather than axis-aligned boxes like in KD Trees.\n",
        "\n",
        "Designed for higher-dimensional data.\n",
        "\n",
        " How it works:\n",
        "\n",
        "At each node:\n",
        "\n",
        "Data is grouped into two clusters (balls) based on centroid and radius.\n",
        "\n",
        "The process recursively builds subtrees.\n",
        "\n",
        " Use case:\n",
        "Better than KD Tree for medium-to-high dimensional data or when using non-Euclidean distance metrics.\n",
        "\n",
        "*7.  When should you use KD Tree vs. Ball Tree ?*\n",
        "\n",
        "Choosing between KD Tree and Ball Tree depends primarily on your dataset‚Äôs dimensionality, size, and the distance metric you plan to use. Here's a clear guide to help you decide :\n",
        "\n",
        "Use KD Tree When:\n",
        "\n",
        "Low-Dimensional Data:\n",
        "\n",
        "Works best when the number of features (dimensions) is < 20.\n",
        "\n",
        "In low dimensions, KD Trees are very fast at nearest-neighbor search.\n",
        "\n",
        "Euclidean Distance:\n",
        "\n",
        "Optimized for axis-aligned splits, which suit Euclidean or similar metrics.\n",
        "\n",
        "Balanced or Moderately Sized Datasets:\n",
        "\n",
        "KD Trees are efficient when the dataset can be recursively split fairly evenly.\n",
        "\n",
        "Use Ball Tree When:\n",
        "\n",
        "Medium to High-Dimensional Data:\n",
        "\n",
        "Ball Trees handle 20+ dimensions better than KD Trees.\n",
        "\n",
        "Still affected by the curse of dimensionality, but less so than KD Trees.\n",
        "\n",
        "Non-Euclidean Distance Metrics:\n",
        "\n",
        "Supports Minkowski, Mahalanobis, and other metrics more efficiently.\n",
        "\n",
        "Unstructured or Unevenly Distributed Data:\n",
        "\n",
        "Ball Trees cluster data in balls (hyperspheres), which can better adapt to irregular distributions.\n",
        "\n",
        "*8. What are the disadvantages of KNN ?*\n",
        "\n",
        "K-Nearest Neighbors (KNN) is simple and intuitive, but it comes with several significant disadvantages that can limit its performance, especially on large or complex datasets.\n",
        "\n",
        "Major Disadvantages of KNN\n",
        "1. Computationally Expensive at Prediction Time\n",
        "KNN is a lazy learner: it does no training, but must compute distances to all training points at prediction time.\n",
        "\n",
        "This makes it slow and inefficient for large datasets.\n",
        "\n",
        "Time Complexity:\n",
        "\n",
        "Training:\n",
        "ùëÇ\n",
        "(\n",
        "1\n",
        ")\n",
        "O(1)\n",
        "\n",
        "Prediction:\n",
        "ùëÇ\n",
        "(\n",
        "ùëõ\n",
        "‚ãÖ\n",
        "ùëë\n",
        ")\n",
        "O(n‚ãÖd), where\n",
        "ùëõ\n",
        "n is the number of training samples,\n",
        "ùëë\n",
        "d is the number of features.\n",
        "\n",
        "2. Sensitive to Irrelevant Features\n",
        "KNN treats all features equally when calculating distance.\n",
        "\n",
        "Irrelevant or noisy features can distort distance metrics, leading to poor predictions.\n",
        "\n",
        "3. Needs Feature Scaling\n",
        "Features on different scales (e.g., height in cm and income in dollars) can skew distance calculations.\n",
        "\n",
        "Normalization or standardization is essential before applying KNN.\n",
        "\n",
        "4. Curse of Dimensionality\n",
        "As the number of features grows:\n",
        "\n",
        "All points tend to become equally distant.\n",
        "\n",
        "It becomes hard to find meaningful neighbors.\n",
        "\n",
        "Performance and accuracy can degrade sharply.\n",
        "\n",
        "5. Storage and Memory Intensive\n",
        "Needs to store the entire training dataset in memory.\n",
        "\n",
        "Not practical for very large datasets or memory-constrained environments.\n",
        "\n",
        "6. No Model Interpretability\n",
        "KNN doesn‚Äôt provide a model with interpretable parameters or coefficients.\n",
        "\n",
        "Hard to explain how a prediction was made beyond ‚Äúit looked like these neighbors.‚Äù\n",
        "\n",
        "7. Struggles with Imbalanced Data\n",
        "If one class dominates, KNN may bias toward the majority class, especially with larger K values.\n",
        "\n",
        "8. Can be Affected by Outliers\n",
        "Outliers in the training data can mislead predictions, particularly with small K values (e.g., K = 1).\n",
        "\n",
        "* 9. How does feature scaling affect KNN ?*\n",
        "\n",
        "Feature scaling has a critical impact on the performance of the K-Nearest Neighbors (KNN) algorithm because KNN relies on distance calculations to identify nearest neighbors.\n",
        "\n",
        "Why Feature Scaling Matters in KNN :\n",
        "\n",
        "KNN uses distance metrics like:\n",
        "\n",
        "Euclidean distance:\n",
        "\n",
        "(\n",
        "ùë•\n",
        "1\n",
        "‚àí\n",
        "ùë¶\n",
        "1\n",
        ")\n",
        "2\n",
        "+\n",
        "(\n",
        "ùë•\n",
        "2\n",
        "‚àí\n",
        "ùë¶\n",
        "2\n",
        ")\n",
        "2\n",
        "+\n",
        "‚Ä¶\n",
        "+\n",
        "(\n",
        "ùë•\n",
        "ùëõ\n",
        "‚àí\n",
        "ùë¶\n",
        "ùëõ\n",
        ")\n",
        "2\n",
        "(x\n",
        "1\n",
        "‚Äã\n",
        " ‚àíy\n",
        "1\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        " +(x\n",
        "2\n",
        "‚Äã\n",
        " ‚àíy\n",
        "2\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        " +‚Ä¶+(x\n",
        "n\n",
        "‚Äã\n",
        " ‚àíy\n",
        "n\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "‚Äã\n",
        "\n",
        "Manhattan, Minkowski, etc.\n",
        "\n",
        "Problem:\n",
        "\n",
        "If features have different scales or units, one feature can dominate the distance calculation, regardless of its importance.\n",
        "\n",
        "*10.  What is PCA (Principal Component Analysis)?*\n",
        "\n",
        "Principal Component Analysis (PCA) is a powerful dimensionality reduction technique used in machine learning and data analysis. It transforms high-dimensional data into a lower-dimensional space while preserving as much variance (information) as possible.\n",
        "\n",
        "Purpose of PCA :\n",
        "\n",
        "Reduce the number of features (dimensions) while retaining important patterns.\n",
        "\n",
        "Remove multicollinearity between features.\n",
        "\n",
        "Speed up algorithms and improve visualization.\n",
        "\n",
        "Help combat the curse of dimensionality (especially useful for KNN).\n",
        "\n",
        "\n",
        "*11.  How does PCA work?*\n",
        "\n",
        "How PCA Works Step-by-Step :\n",
        "\n",
        "1. Standardize the Data\n",
        "Center each feature by subtracting its mean so that the data has zero mean.\n",
        "\n",
        "Often, scale to unit variance (standard deviation = 1) so all features contribute equally.\n",
        "\n",
        "2. Compute the Covariance Matrix\n",
        "Calculate the covariance matrix\n",
        "ùê∂\n",
        "C of the standardized data.\n",
        "\n",
        "The covariance matrix shows how features vary together:\n",
        "\n",
        "ùê∂\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚àí\n",
        "1\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë•\n",
        "Àâ\n",
        ")\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë•\n",
        "Àâ\n",
        ")\n",
        "ùëá\n",
        "C=\n",
        "n‚àí1\n",
        "1\n",
        "‚Äã\n",
        "  \n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " (x\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "x\n",
        "Àâ\n",
        " )(x\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "x\n",
        "Àâ\n",
        " )\n",
        "T\n",
        "\n",
        "If features are independent, covariances are zero.\n",
        "\n",
        "3. Calculate Eigenvalues and Eigenvectors\n",
        "Solve the equation:\n",
        "\n",
        "ùê∂\n",
        "ùë£\n",
        "=\n",
        "ùúÜ\n",
        "ùë£\n",
        "Cv=Œªv\n",
        "where:\n",
        "\n",
        "ùë£\n",
        "v = eigenvector (principal component direction)\n",
        "\n",
        "ùúÜ\n",
        "Œª = eigenvalue (variance explained by this component)\n",
        "\n",
        "Each eigenvector points in a direction of maximal variance.\n",
        "\n",
        "4. Sort Eigenvectors by Eigenvalues\n",
        "Rank the eigenvectors by their eigenvalues from largest to smallest.\n",
        "\n",
        "The top eigenvectors correspond to the directions that explain the most variance.\n",
        "\n",
        "5. Select Top K Principal Components\n",
        "Choose the first\n",
        "ùëò\n",
        "k eigenvectors that capture the majority of variance (e.g., 95%).\n",
        "\n",
        "These form the new reduced feature space.\n",
        "\n",
        "6. Project Data onto Principal Components\n",
        "Transform original data\n",
        "ùëã\n",
        "X onto the new space:\n",
        "\n",
        "ùëã\n",
        "PCA\n",
        "=\n",
        "ùëã\n",
        "√ó\n",
        "ùëä\n",
        "X\n",
        "PCA\n",
        "‚Äã\n",
        " =X√óW\n",
        "where\n",
        "ùëä\n",
        "W is the matrix of selected eigenvectors.\n",
        "\n",
        "*12.  What is the geometric intuition behind PCA ?*\n",
        "\n",
        "Geometric Intuition of PCA\n",
        "Imagine your data points scattered in a high-dimensional space (e.g., 2D or 3D for easy visualization):\n",
        "\n",
        "1. Data Cloud in Space\n",
        "Your dataset is like a cloud of points in space.\n",
        "\n",
        "Each axis represents one feature (dimension).\n",
        "\n",
        "The points are spread out unevenly, with some directions having more spread (variance) than others.\n",
        "\n",
        "2. Finding the Direction of Maximum Variance\n",
        "PCA tries to find a new axis (line) that best fits the data in terms of spread.\n",
        "\n",
        "This new axis is the direction along which the data varies the most.\n",
        "\n",
        "Think of it as the direction along which the cloud of points is stretched out the furthest.\n",
        "\n",
        "3. First Principal Component\n",
        "The first principal component is this line of maximum variance.\n",
        "\n",
        "If you were to project all points onto this line, the projections would have the largest possible spread compared to any other direction.\n",
        "\n",
        "It captures the most important pattern or information in the data.\n",
        "\n",
        "4. Subsequent Components\n",
        "The second principal component is another axis, orthogonal (at right angles) to the first, which captures the next highest variance.\n",
        "\n",
        "This ensures new components add new, non-redundant information.\n",
        "\n",
        "You can think of this as the second-best line along which the data spreads out, but perpendicular to the first.\n",
        "\n",
        "5. Dimensionality Reduction\n",
        "By choosing just the top\n",
        "ùëò\n",
        "k principal components, you flatten your data cloud onto a lower-dimensional subspace (e.g., a plane or a line).\n",
        "\n",
        "This retains most of the shape and structure of the original data but with fewer dimensions.\n",
        "\n",
        "\n",
        "*13.  What is the difference between Feature Selection and Feature Extraction ?*\n",
        "\n",
        "Feature Selection vs. Feature Extraction :\n",
        "\n",
        "| Aspect                | Feature Selection                                                                                                                 | Feature Extraction                                                                                 |\n",
        "| --------------------- | --------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |\n",
        "| **What it does**      | Selects a **subset of original features**                                                                                         | Creates **new features** by transforming original ones                                             |\n",
        "| **Original features** | **Kept as-is** (some dropped)                                                                                                     | **Combined or transformed** into new features                                                      |\n",
        "| **Goal**              | Remove irrelevant or redundant features                                                                                           | Reduce dimensionality by capturing essential information                                           |\n",
        "| **Interpretability**  | Usually easier to interpret (features stay the same)                                                                              | Harder to interpret (new features are combinations)                                                |\n",
        "| **Examples**          | - Filter methods (correlation, chi-square) <br> - Wrapper methods (recursive feature elimination) <br> - Embedded methods (Lasso) | - Principal Component Analysis (PCA) <br> - Linear Discriminant Analysis (LDA) <br> - Autoencoders |\n",
        "| **When to use**       | When original features are meaningful and you want to keep them                                                                   | When dimensionality is very high and you want to compress info                                     |\n",
        "| **Effect on data**    | Dataset dimension is reduced by **dropping** features                                                                             | Dataset dimension is reduced by **transforming** features                                          |\n",
        "\n",
        "\n",
        "*14.  What are Eigenvalues and Eigenvectors in PCA? *\n",
        "\n",
        "Eigenvectors and Eigenvalues :\n",
        "\n",
        "1. Eigenvectors\n",
        "Eigenvectors are special vectors that, when multiplied by a matrix (like a covariance matrix), do not change direction‚Äîonly their magnitude might change.\n",
        "\n",
        "In PCA, each eigenvector represents a principal component ‚Äî a direction in feature space along which the data varies.\n",
        "\n",
        "Geometrically, these eigenvectors define the new axes (coordinate system) for your data.\n",
        "\n",
        "2. Eigenvalues\n",
        "Each eigenvector has a corresponding eigenvalue, a scalar that indicates how much variance (information) there is along that eigenvector.\n",
        "\n",
        "Larger eigenvalues mean the corresponding eigenvector captures more variance in the data.\n",
        "\n",
        "In PCA, eigenvalues tell you the importance of each principal component.\n",
        "\n",
        "In PCA Context\n",
        "Compute the covariance matrix of your (usually standardized) data.\n",
        "\n",
        "Solve the equation:\n",
        "\n",
        "ùê∂\n",
        "ùë£\n",
        "=\n",
        "ùúÜ\n",
        "ùë£\n",
        "Cv=Œªv\n",
        "where:\n",
        "\n",
        "ùê∂\n",
        "C = covariance matrix\n",
        "\n",
        "ùë£\n",
        "v = eigenvector\n",
        "\n",
        "ùúÜ\n",
        "Œª = eigenvalue\n",
        "\n",
        "This means that applying the covariance matrix to\n",
        "ùë£\n",
        "v simply scales it by\n",
        "ùúÜ\n",
        "Œª.\n",
        "\n",
        "*15.  How do you decide the number of components to keep in PCA?*\n",
        "\n",
        "Deciding how many principal components to keep in PCA is a key step because it balances dimensionality reduction and information retention.\n",
        "\n",
        "How to Decide the Number of Components in PCA ;\n",
        "\n",
        "1. Explained Variance Ratio\n",
        "Each principal component explains a certain percentage of the total variance in the data.\n",
        "\n",
        "The explained variance ratio tells you how much information each component captures.\n",
        "\n",
        "You typically look at the cumulative explained variance to decide how many components to keep.\n",
        "\n",
        "2. Choose Components Based on Variance Threshold\n",
        "Common practice: choose the minimum number of components that explain at least 90% to 95% of the variance.\n",
        "\n",
        "Example:\n",
        "\n",
        "First 3 PCs explain 92% of variance ‚Üí keep 3 components.\n",
        "\n",
        "Keeping more might add little additional information but increase complexity.\n",
        "\n",
        "3. Scree Plot (Elbow Method)\n",
        "Plot the explained variance against the number of components.\n",
        "\n",
        "Look for an ‚Äúelbow‚Äù point where adding more components yields diminishing returns.\n",
        "\n",
        "Components after the elbow contribute very little additional variance.\n",
        "\n",
        "4. Domain Knowledge\n",
        "Sometimes, practical or domain-specific considerations affect how many components to keep.\n",
        "\n",
        "For visualization, you might pick 2 or 3 components regardless.\n",
        "\n",
        "For modeling, balancing accuracy and complexity matters.\n",
        "\n",
        "5. Cross-Validation\n",
        "Evaluate downstream model performance (e.g., classification accuracy) using different numbers of components.\n",
        "\n",
        "Choose the number that gives the best trade-off between performance and dimensionality.\n",
        "\n",
        "*16. Can PCA be used for classification?*\n",
        "\n",
        "PCA itself is not a classification algorithm, but it can be very useful as a preprocessing step for classification tasks.\n",
        "\n",
        "How PCA Relates to Classification :\n",
        "\n",
        "1. PCA for Dimensionality Reduction Before Classification\n",
        "PCA reduces the number of features by projecting data into a lower-dimensional space while preserving most of the variance.\n",
        "\n",
        "This can help:\n",
        "\n",
        "Speed up classification algorithms.\n",
        "\n",
        "Reduce noise and irrelevant information.\n",
        "\n",
        "Mitigate overfitting by simplifying the feature space.\n",
        "\n",
        "2. Improving Classifier Performance\n",
        "By removing redundant or less informative features, PCA can make classes more separable in the transformed space.\n",
        "\n",
        "Some classifiers (like KNN or SVM) benefit from a smaller, cleaner feature set.\n",
        "\n",
        "3. Limitations\n",
        "PCA is unsupervised ‚Äî it doesn't use class labels when finding components.\n",
        "\n",
        "It focuses on variance, not on class separation.\n",
        "\n",
        "Therefore, PCA might not always find the directions that best discriminate between classes.\n",
        "\n",
        "4. Alternatives: Supervised Dimension Reduction\n",
        "Methods like Linear Discriminant Analysis (LDA) take class labels into account and aim to maximize class separability.\n",
        "\n",
        "Sometimes LDA works better than PCA for classification.\n",
        "\n",
        "\n",
        "*17.  What are the limitations of PCA? *\n",
        "\n",
        "Limitations of PCA\n",
        "1. Linearity Assumption\n",
        "PCA assumes that the principal components are linear combinations of the original features.\n",
        "\n",
        "It cannot capture non-linear relationships in the data.\n",
        "\n",
        "For complex patterns, non-linear techniques (e.g., Kernel PCA, t-SNE, UMAP) may be better.\n",
        "\n",
        "2. Unsupervised Method\n",
        "PCA does not use class labels or any target information.\n",
        "\n",
        "It maximizes variance, but the directions of maximum variance are not always the most relevant for classification or prediction tasks.\n",
        "\n",
        "Important discriminative features might have low variance and be ignored.\n",
        "\n",
        "3. Sensitivity to Scaling\n",
        "PCA is sensitive to the scale of features.\n",
        "\n",
        "Features with larger scales dominate variance unless data is properly standardized before applying PCA.\n",
        "\n",
        "4. Interpretability Issues\n",
        "Principal components are linear combinations of all original features.\n",
        "\n",
        "This can make it hard to interpret the transformed features, especially for domain experts.\n",
        "\n",
        "5. Loss of Information\n",
        "Dimensionality reduction inevitably causes some loss of information.\n",
        "\n",
        "Choosing too few components may omit important details, hurting model performance.\n",
        "\n",
        "6. Outlier Sensitivity\n",
        "PCA can be sensitive to outliers, which can distort the directions of maximum variance.\n",
        "\n",
        "\n",
        "*18.  How do KNN and PCA complement each other ?*\n",
        "\n",
        "KNN and PCA can work really well together because their strengths complement each other, especially when dealing with high-dimensional data.\n",
        "\n",
        "How KNN and PCA Complement Each Other :\n",
        "\n",
        "1. PCA Reduces Dimensionality for KNN\n",
        "KNN relies on distance calculations (e.g., Euclidean distance) to find nearest neighbors.\n",
        "\n",
        "In high-dimensional spaces, distances become less meaningful due to the curse of dimensionality.\n",
        "\n",
        "PCA reduces the number of features by projecting data into a lower-dimensional space that retains most variance.\n",
        "\n",
        "This makes the distance computations in KNN more meaningful and efficient.\n",
        "\n",
        "2. PCA Helps Remove Noise and Redundancy\n",
        "High-dimensional data often contains noisy or redundant features.\n",
        "\n",
        "PCA captures the most important patterns and discards noise.\n",
        "\n",
        "KNN benefits from this by focusing on cleaner, more informative features for neighbor selection.\n",
        "\n",
        "3. Faster KNN Computation\n",
        "With fewer dimensions, KNN requires less computation for distance calculations.\n",
        "\n",
        "This improves KNN‚Äôs speed and scalability.\n",
        "\n",
        "4. Improves KNN Performance\n",
        "By reducing irrelevant features and noise, PCA can help KNN achieve better accuracy and generalization.\n",
        "\n",
        "*19.  How does KNN handle missing values in a dataset?*\n",
        "\n",
        "How KNN Deals with Missing Values\n",
        "1. KNN Cannot Directly Handle Missing Values\n",
        "KNN relies on computing distances between points.\n",
        "\n",
        "Missing values mean incomplete feature vectors, so the distance calculation breaks down.\n",
        "\n",
        "2. Common Strategies to Handle Missing Data Before KNN\n",
        "a) Imputation\n",
        "Fill in missing values before applying KNN.\n",
        "\n",
        "Common imputation methods:\n",
        "\n",
        "Mean/Median Imputation: Replace missing values with the mean or median of the feature.\n",
        "\n",
        "KNN Imputation: Use KNN itself to estimate missing values by looking at nearest neighbors with complete data.\n",
        "\n",
        "Model-Based Imputation: Use regression or other predictive models.\n",
        "\n",
        "b) Remove Samples or Features\n",
        "Drop rows (samples) with missing values if only a few are missing.\n",
        "\n",
        "Drop features (columns) with too many missing values.\n",
        "\n",
        "3. KNN Variants That Handle Missing Values\n",
        "Some modified KNN algorithms can handle missing values by:\n",
        "\n",
        "Computing distances using only the features present in both samples (partial distance).\n",
        "\n",
        "Weighting the distance by the number of available features.\n",
        "\n",
        "These are less common and require careful implementation.\n",
        "\n",
        "*20.  What are the key differences between PCA and Linear Discriminant Analysis (LDA)?*\n",
        "\n",
        "Both PCA and LDA are popular dimensionality reduction techniques, but they have different goals and approaches. Here are the key differences:\n",
        "\n",
        "| Aspect                   | Principal Component Analysis (PCA)                                | Linear Discriminant Analysis (LDA)                                         |\n",
        "| ------------------------ | ----------------------------------------------------------------- | -------------------------------------------------------------------------- |\n",
        "| **Goal**                 | Find directions that **maximize variance** in data (unsupervised) | Find directions that **maximize class separability** (supervised)          |\n",
        "| **Supervision**          | **Unsupervised** ‚Äî ignores class labels                           | **Supervised** ‚Äî uses class labels                                         |\n",
        "| **Focus**                | Captures overall data structure                                   | Focuses on differences between classes                                     |\n",
        "| **Criteria optimized**   | Maximizes total variance                                          | Maximizes ratio of **between-class variance** to **within-class variance** |\n",
        "| **Output components**    | Principal components ordered by variance explained                | Linear discriminants ordered by class separability                         |\n",
        "| **Dimensionality limit** | Can extract up to $\\min(n_{samples}, n_{features})$ components    | Limited to $\\leq (C - 1)$ components, where $C$ = number of classes        |\n",
        "| **Typical use cases**    | Data compression, visualization, noise reduction                  | Classification, feature extraction for supervised tasks                    |\n",
        "| **Interpretability**     | Components are directions of max variance                         | Components maximize class separation                                       |\n"
      ],
      "metadata": {
        "id": "B0vSsgTe1TBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Train a KNN Classifier on the Iris dataset and print model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize KNN classifier with k=5 neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"KNN Classifier Accuracy on Iris dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "iLRyxxfD8Ypo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22.  Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize KNN regressor with k=5 neighbors\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"KNN Regressor Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "e2h6x9P58j_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23.  Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize KNN with Euclidean distance (default)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# Initialize KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy with Euclidean distance: {accuracy_euclidean:.2f}\")\n",
        "print(f\"Accuracy with Manhattan distance: {accuracy_manhattan:.2f}\")\n"
      ],
      "metadata": {
        "id": "_UD5cD338tb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24.  Train a KNN Classifier with different values of K and visualize decision boundarie\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris data\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # Use only first two features for visualization\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create color maps\n",
        "cmap_light = plt.cm.Pastel2\n",
        "cmap_bold = plt.cm.Set1\n",
        "\n",
        "# Values of K to try\n",
        "k_values = [1, 3, 7, 15]\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "for i, k in enumerate(k_values, 1):\n",
        "    # Train KNN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Create mesh grid for plotting decision boundaries\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "    # Predict class for each point in the mesh grid\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    plt.subplot(2, 2, i)\n",
        "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "    # Plot training points\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold, edgecolor='k', s=50)\n",
        "    plt.title(f\"KNN with k={k}\")\n",
        "    plt.xlabel(iris.feature_names[0])\n",
        "    plt.ylabel(iris.feature_names[1])\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NyeAP9uk81NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25.  Apply Feature Scaling before training a KNN model and compare results with unscaled data\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Without scaling ---\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --- With scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "dBTN7Az99EcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26.  Train a PCA model on synthetic data and print the explained variance ratio for each component\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, _ = make_classification(n_samples=200, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Print explained variance ratio for each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "for i, var_ratio in enumerate(explained_variance, 1):\n",
        "    print(f\"Principal Component {i}: Explained Variance Ratio = {var_ratio:.4f}\")\n"
      ],
      "metadata": {
        "id": "r_qP7gC09RMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27.  Apply PCA before training a KNN Classifier and compare accuracy with and without PCA\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (important for PCA and KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Without PCA ---\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "acc_without_pca = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# --- With PCA ---\n",
        "pca = PCA(n_components=2)  # Reduce to 2 components for illustration\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_with_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(f\"Accuracy without PCA: {acc_without_pca:.2f}\")\n",
        "print(f\"Accuracy with PCA: {acc_with_pca:.2f}\")\n"
      ],
      "metadata": {
        "id": "KLEdxl5q9c4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28.  Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best cross-validation accuracy\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Evaluate best model on test data\n",
        "best_knn = grid_search.best_estimator_\n",
        "test_accuracy = best_knn.score(X_test, y_test)\n",
        "print(f\"Test set accuracy with best parameters: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "RJ_djSvQ-GKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29.  Train a KNN Classifier and check the number of misclassified samples\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate number of misclassified samples\n",
        "num_misclassified = (y_test != y_pred).sum()\n",
        "print(f\"Number of misclassified samples: {num_misclassified}\")\n"
      ],
      "metadata": {
        "id": "RsXVdhPv-P0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9wiH5io1QBW"
      },
      "outputs": [],
      "source": [
        "# 30.  Train a PCA model and visualize the cumulative explained variance.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# KNN with uniform weights (all neighbors weighted equally)\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "knn_uniform.fit(X_train, y_train)\n",
        "y_pred_uniform = knn_uniform.predict(X_test)\n",
        "acc_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "# KNN with distance weights (closer neighbors weighted more)\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "knn_distance.fit(X_train, y_train)\n",
        "y_pred_distance = knn_distance.predict(X_test)\n",
        "acc_distance = accuracy_score(y_test, y_pred_distance)\n",
        "\n",
        "print(f\"Accuracy with uniform weights: {acc_uniform:.2f}\")\n",
        "print(f\"Accuracy with distance weights: {acc_distance:.2f}\")\n"
      ],
      "metadata": {
        "id": "YoiMZys--iYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32.  Train a KNN Regressor and analyze the effect of different K values on performance\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=15, random_state=42)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Try different values of K\n",
        "k_values = range(1, 31)\n",
        "mse_values = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_reg.fit(X_train, y_train)\n",
        "    y_pred = knn_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_values.append(mse)\n",
        "\n",
        "# Plot K vs MSE\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, mse_values, marker='o')\n",
        "plt.title(\"Effect of K on KNN Regressor Performance\")\n",
        "plt.xlabel(\"Number of Neighbors (K)\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6Q8WjmrT-rs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33.  Implement KNN Imputation for handling missing values in a dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# Example dataset with missing values (np.nan)\n",
        "X = np.array([\n",
        "    [1.0, 2.0, np.nan],\n",
        "    [3.0, np.nan, 1.0],\n",
        "    [np.nan, 0.0, 2.0],\n",
        "    [4.0, 2.0, 3.0],\n",
        "    [5.0, 3.0, np.nan]\n",
        "])\n",
        "\n",
        "# Initialize KNNImputer with k neighbors (default k=5)\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "\n",
        "# Fit and transform the dataset to impute missing values\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "print(\"Original data with missing values:\")\n",
        "print(X)\n",
        "print(\"\\nData after KNN imputation:\")\n",
        "print(X_imputed)\n"
      ],
      "metadata": {
        "id": "jfuTXpLY-21F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34.  Train a PCA model and visualize the data projection onto the first two principal components\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Scale the data (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA to reduce to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot the projection\n",
        "plt.figure(figsize=(8,6))\n",
        "colors = ['navy', 'turquoise', 'darkorange']\n",
        "\n",
        "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
        "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n",
        "                color=color, lw=2, label=target_name, alpha=0.7)\n",
        "\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('PCA Projection of Iris Dataset')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Tu0IG6QW--3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35.  Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance\n",
        "\n",
        "import time\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a helper function to train and time the model\n",
        "def train_and_evaluate(algorithm):\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, algorithm=algorithm)\n",
        "    start_time = time.time()\n",
        "    knn.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    y_pred = knn.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy, train_time\n",
        "\n",
        "# Train using KD Tree\n",
        "acc_kd, time_kd = train_and_evaluate('kd_tree')\n",
        "\n",
        "# Train using Ball Tree\n",
        "acc_ball, time_ball = train_and_evaluate('ball_tree')\n",
        "\n",
        "print(f\"KD Tree -> Accuracy: {acc_kd:.2f}, Training Time: {time_kd:.6f} seconds\")\n",
        "print(f\"Ball Tree -> Accuracy: {acc_ball:.2f}, Training Time: {time_ball:.6f} seconds\")\n"
      ],
      "metadata": {
        "id": "pFBb4Anz_GPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36.  Train a PCA model on a high-dimensional dataset and visualize the Scree plot\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a high-dimensional synthetic dataset\n",
        "X, _ = make_classification(n_samples=300, n_features=50, n_informative=20, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio for each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Scree plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='b')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.xticks(range(1, len(explained_variance) + 1))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ig6yQtFi_Nn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37.  Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Print classification report (includes precision, recall, f1-score)\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "WG4fXX2P_W_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 38.  Train a PCA model and analyze the effect of different numbers of components on accuracy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Try different PCA components and record accuracy\n",
        "component_range = range(1, X.shape[1] + 1)  # 1 to number of features\n",
        "accuracies = []\n",
        "\n",
        "for n_components in component_range:\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "    # Train KNN on PCA-transformed data\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "    y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(component_range, accuracies, marker='o')\n",
        "plt.title('Effect of Number of PCA Components on KNN Accuracy')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(component_range)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BSB31XNG_fGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 39.  Train a KNN Classifier with different leaf_size values and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "leaf_sizes = [1, 10, 20, 30, 40, 50]\n",
        "accuracies = []\n",
        "\n",
        "for leaf_size in leaf_sizes:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, leaf_size=leaf_size, algorithm='kd_tree')\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Leaf size: {leaf_size}, Accuracy: {acc:.2f}\")\n",
        "\n",
        "# Optionally, plot accuracy vs leaf_size\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(leaf_sizes, accuracies, marker='o')\n",
        "plt.title('Effect of leaf_size on KNN Accuracy (kd_tree)')\n",
        "plt.xlabel('leaf_size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fpSs374v_nLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40.  Train a PCA model and visualize how data points are transformed before and after PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Scale the data (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA and transform data to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot original data (first two features)\n",
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "colors = ['navy', 'turquoise', 'darkorange']\n",
        "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
        "    plt.scatter(X[y == i, 0], X[y == i, 1], color=color, lw=2, label=target_name, alpha=0.7)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Original Data (First Two Features)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot PCA transformed data (first two principal components)\n",
        "plt.subplot(1, 2, 2)\n",
        "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
        "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, lw=2, label=target_name, alpha=0.7)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Transformed Data (2 Components)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XgrLAKvC_xvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 41.  Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split dataset into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred, target_names=wine.target_names))\n"
      ],
      "metadata": {
        "id": "tCzco1F9_6Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 42.  Train a KNN Regressor and analyze the effect of different distance metrics on prediction error\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=20, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Distance metrics to test\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "mse_scores = []\n",
        "\n",
        "for metric in metrics:\n",
        "    knn = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"Distance metric: {metric}, Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Optional: Bar plot to visualize MSE for each metric\n",
        "plt.bar(metrics, mse_scores, color=['skyblue', 'salmon'])\n",
        "plt.title('Effect of Distance Metric on KNN Regression Error')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xhtKeuA-A9hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 43.  Train a KNN Classifier and evaluate using ROC-AUC score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_probs = knn.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC\n",
        "auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {auc:.3f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "plt.plot(fpr, tpr, label=f'KNN (AUC = {auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VTKwQdDUBIcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 44.  Train a PCA model and visualize the variance captured by each principal component\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Scale features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot variance explained by each component\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='teal')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Explained Ratio')\n",
        "plt.title('Variance Explained by Each Principal Component')\n",
        "plt.xticks(range(1, len(explained_variance) + 1))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q369u0gYBRRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 45.  Train a KNN Classifier and perform feature selection before training\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Selection: Select top 2 features\n",
        "selector = SelectKBest(score_func=f_classif, k=2)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Scale features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "HZ3Su0ttBbdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 46.  Train a PCA model and visualize the data reconstruction error after reducing dimensions\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Try different numbers of components and calculate reconstruction error\n",
        "max_components = X.shape[1]\n",
        "errors = []\n",
        "\n",
        "for n_components in range(1, max_components + 1):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_reduced = pca.fit_transform(X_scaled)\n",
        "    X_reconstructed = pca.inverse_transform(X_reduced)\n",
        "    mse = mean_squared_error(X_scaled, X_reconstructed)\n",
        "    errors.append(mse)\n",
        "\n",
        "# Plot reconstruction error vs number of components\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, max_components + 1), errors, marker='o', color='red')\n",
        "plt.xlabel('Number of PCA Components')\n",
        "plt.ylabel('Mean Squared Reconstruction Error')\n",
        "plt.title('PCA Reconstruction Error vs Number of Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xOWoAArsBkqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 47.  Train a KNN Classifier and visualize the decision boundary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # Use first two features for 2D plot\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Create a mesh grid for plotting decision boundaries\n",
        "h = 0.02  # step size in the mesh\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Predict class for each point in the mesh grid\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary and training points\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Set1)\n",
        "colors = ['red', 'green', 'blue']\n",
        "for idx, color in enumerate(colors):\n",
        "    plt.scatter(X_train_scaled[y_train == idx, 0], X_train_scaled[y_train == idx, 1],\n",
        "                c=color, label=iris.target_names[idx], edgecolor='k')\n",
        "plt.xlabel('Feature 1 (standardized)')\n",
        "plt.ylabel('Feature 2 (standardized)')\n",
        "plt.title('KNN Decision Boundary (k=5)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QfOHLrPkByPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 48.  Train a PCA model and analyze the effect of different numbers of components on data variance.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Scale data before PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA with all components\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "\n",
        "# Plot cumulative variance vs number of components\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Effect of Number of PCA Components on Variance Captured')\n",
        "plt.xticks(range(1, len(cumulative_variance) + 1))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z1M67apsCDwX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}